

Transformer Layers:

1. Tokenization layer. Tokens and tokenization process is fixed.
 https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B/tree/main
"vocab_size": 50254 defined in https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B/blob/main/config.json

2. Embedding layer.
 Embedding is created by randomly generated real numbers of size 256.
 Then the numbers are compressed using haar wavelet to the size of 16.
 Then cmor wavelet is applied to transform the weights once again. 
 The result is complex numbers of size 16 for each embedding. 
 Vocabulary size 50254.
 Embedding dimension: 16.
 Embedding type: Vec<Complex<f64>>

3. Positional encoding layer.
Rotary Positional Encoding is used as positional encoding algrorithms. 

 Î¸i = position/10000 exp(2i/d)
 Position - is the token id number in the sentence. E.g. if we have a sentence: "I have a dream" and 5 token ids: [500, 12, 34, 44, 25]. 
 Then, the position of the token 500 is 0, for token 12 is 1, for token 34 is 3 etc. 
 i - is the index in the token embedding itself. 
 In other word, the position is the position of embedding in the sentence and the index i is the index in the embedding itself. 
 d - is the dimension of embedding. In our case is 16. 
 
3. Masked Self-Attention-Layer consists of 4 masked attention heads.
 If input has a size [30][16], after passing the self-attenion layer it will still have the same size [30][16];

4. Add and Norm: RMS Norm is applied here.

5. FeedForward network with weight matrices for hidden layer [16][64], output [64][16]; 

6. Add and Norm: RMS Norm is applied here.

7. Linear layer. Just matrix multiplication of weights and bias adding. output[30][50254]

8. Softmax layer. The output is [30][50254] for the input of 30 token ids. 


----------------------------------------------------------------------------------
Traning mode: 
Training Mode: Structure of Transformer Model

Encoder Layer:
The encoder layer consists of multiple attention heads, e.g., 4 heads.
The encoder processes the input tokens of size 
[30][512] (30 tokens, each with a 512-dimensional embedding).
The output of the encoder layer is also of size 
[30][512]. Denote this output as encoder output 1.

Decoder Layer:
The decoder layer consists of multiple masked multi-head attention heads (equal to the number of attention heads in the encoder, e.g., 4).
The decoder takes the target tokens (e.g., translations) as input, processes them through the masked multi-head attention heads,
 and outputs a representation. Denote this output as decoder output 1.

Encoder-Decoder Attention Layer:
The encoder-decoder attention layer is located inside the decoder layer, following the masked multi-head attention layer.

Function:
After the masked self-attention in the decoder, the encoder-decoder attention heads attend to the encoder output 1 (defined in step 1, the output of the encoder layer).
These layers are where the decoder "communicates" with the encoder.
Inputs:

The Query (Q): Comes from the output of the masked multi-head attention layer (decoder output 1).
The Key (K) and Value (V): Both come from the encoder output 1.
ð¾ and ð‘‰ are identical and have size [30][512] (encoder's output).

How Encoder-Decoder Attention Head Works
Attention Mechanism:

Each encoder-decoder attention head processes the 
Q, ð¾, and ð‘‰ using the following formula:

Correct Workflow of the Encoder-Decoder Attention Head
Inputs to the Attention Head:

Query (ð‘„): Comes from the decoder output (output of the masked self-attention layer).
Key (ð¾): Comes from the encoder output.
Value (ð‘‰): Comes from the encoder output.
These inputs initially have dimensions:

ð‘„: [ð¿tgt,ð‘‘model][L tgtâ€‹ ,d modelâ€‹ ] (e.g., [25][512]).
ð¾, ð‘‰: [ð¿src,ð‘‘model][L srcâ€‹ ,d modelâ€‹ ] (e.g., [30][512]).

Projection to ð‘„ð‘ž, ð¾ð‘˜, and ð‘‰ð‘£ 
â€‹
 : Each of these inputs is multiplied by learnable weight matrices (ð‘Šð‘„, ð‘Šð¾, ð‘Šð‘‰) to produce the transformed versions
 (ð‘„ð‘ž, ð¾k, ð‘‰ð‘£):

ð‘„ð‘ž=ð‘„â‹…ð‘Šq,  where ð‘Šqâˆˆð‘… ð‘‘modelÃ—ð‘‘head.
ð¾ð‘˜=ð¾â‹…ð‘Šk, where ð‘Šk âˆˆ ð‘… dmodelÃ—d head.
ð‘‰ð‘£= =ð‘‰â‹…ð‘Šv, where ð‘Šv âˆˆ ð‘… ð‘‘modelÃ—ð‘‘head.

After this step, the transformed 
ð‘„ð‘ž,Kk, and ð‘‰ð‘£ have dimensions:Qq : [ð¿tgt,ð‘‘head][Lâ€‹ ,d headâ€‹ ] [25][64], assuming ð‘‘head=ð‘‘model/num_heads.
ð¾ð‘˜: [ð¿src,ð‘‘head][L srcâ€‹ ,d headâ€‹ ] (e.g., [30][64]).
ð‘‰ð‘£: [ð¿src,ð‘‘head][L srcâ€‹ ,d headâ€‹ ] (e.g., [30][64]).

Weight matrices are still there: Wq, Wk, Wv
Qq = Q * Wq
Kk = K * Wk 
Vv = V * Wv
Attention(ð‘„q,ð¾k,ð‘‰v)=softmax(ð‘„qð¾kð‘‡/ð‘‘ð‘˜)ð‘‰v
Attention(Qq,Kk,Vv) = [25][512]

Qq: Decoder query vector ([25][512], where 25 is the target sequence length).
ð¾,ð‘‰: Encoder's key and value vectors ([30][512], where 30 is the source sequence length).

Output:
The attention mechanism produces a contextualized representation for each target token, allowing the decoder to focus on the relevant parts of the source sequence.
Multiple Heads:

Like the encoder and masked attention, there can be multiple encoder-decoder attention heads, each focusing on different parts of the source-target relationship.

If we have multiple encoder-decoder attention heads insider encoder-decoder block then:
Each attention head produces an output of size [25][64], where 64=512/8 (if we have 8 encoder-decoder attention heads).
Then the output of all head is concatenated to the original array [25][512]. Just like in normal attention head.

If we have multiple encoder-decoder blocks, then the output of the last encoder-decoder layer is forwarded as Query (Q).
 The K and V remain the same as input in the first encoder-decoder block. 

Loss Function: At each step, compute the cross-entropy loss between the predicted distribution (softmax output) and the ground truth token.



Derivative of masked attention head:

A = Q*KT/sqtr(dk)
S = sigma(A)
O = S * V


# Gradiet of Wq
dl/dWq = dl/dO * dO/dS * dS/dA * dA/dQ * dQ/dWq

dl/dO = Gt
dO/dS = VT
dS/dA = gradient sigma(A)
dA/dQ = KT/sqtr(dk)
dQ/dWq = XT

Example with dimensions: 
Gt (2, 4)
X (2, 5)
W (5, 4)
Q = 2,5 * 5,4 = 2,4
K = 2,5 * 5,4 = 2,4
V = 2,5 * 5,4 = 2,4

dl/dWq = dl/dO * dO/dS * dS/dA * dA/dQ * dQ/dWq
dl/dWq = XT * (Gt * VT * ds/dA * KT/sqtr(dk))

dl/dWq = 2,5 * (2,4 * 4,2 * 2,2 * 4,2) = 2,5

The order of matrix multiplication can be explained:

dl/dWq = (((dl/dO * dO/dS) * dS/dA) * dA/dQ) * dQ/dWq

Multiplication is FROM RIGHT TO LEFT:
dl/dWq = dQ/dWq * (dA/dQ * ( dS/dA * (dl/dO * dO/dS)))

=> dl/dWq = XT * (KT/sqtr(dk) * (ds/dA * (Gt * VT)))
=> dl/dWq = 4 * (3 * (2 * (1)))

1. (Gt * VT) = 2,4 * 4, 2 = 2,2
2. ds/dA * 1 = 2,2 * 2,2 = 2,2
3. KT/sqtr(dk) * 2 = 4,2 2,2 = 4,2
4. 4 * 3 = XT * 3 = 5,2 * 4,2 = 5,2 * 2, 4 = 5,4

#Gradient of Wk
A = Q*KT/sqtr(dk)
S = sigma(A)
O = S * V

dl/dWk = dl/dO * dO/dS * dS/dA * dA/dKT * dKT/dWk

dl/dO = Gt
dO/dS = VT
dS/dA = gradient sigma(A)
dA/dKT = Q/sqtr(dk)
dKT/dWk = XT

Example with dimensions: 
Gt (2, 4)
X (2, 5)
W (5, 4)
Q = 2,5 * 5,4 = 2,4
K = 2,5 * 5,4 = 2,4
V = 2,5 * 5,4 = 2,4

dl/dWk = dl/dO * dO/dS * dS/dA * dA/dKT * dKT/dWk

dl/dWk = (((dl/dO * dO/dS) * dS/dA) * dA/dKT) * dKT/dWk

Multiplication is FROM RIGHT TO LEFT:
dl/dWk = dKT/dWk * (dA/dKT * (dS/dA * (dl/dO * dO/dS)))

=> dl/dWk = XT * (QT/sqtr(dk) * (ds/dA * (Gt * VT)))
=> dl/dWq = 4 * (3 * (2 * (1)))

1. (Gt * VT) = 2,4 * 4, 2 = 2,2
2. ds/dA * 1 = 2,2 * 2,2 = 2,2
3. QT/sqtr(dk) * 2 = 4,2 * 2,2 = 4,2
4. 4 * 3 = XT * 3 = 5,2 * 4,2 = 5,2 * 2, 4 = 5,4


#Gradient of Wv
A = Q*KT/sqtr(dk)
S = sigma(A)
O = S * V

dl/dWv = dl/dO * dO/dV * dV/dWv

dl/dO = Gt
dO/dS = VT
dV/dWv = XT

Example with dimensions: 
Gt (2, 4)
X (2, 5)
W (5, 4)
Q = 2,5 * 5,4 = 2,4
K = 2,5 * 5,4 = 2,4
V = 2,5 * 5,4 = 2,4

dl/dWv = dl/dO * dO/dV * dV/dWv

dl/dWv = (dl/dO * dO/dV) * dV/dWv

Multiplication is FROM RIGHT TO LEFT:
dl/dWv = dV/dWv * (dl/dO * dO/dV)

=> dl/dWv = XT * (Gt * ST)
=> dl/dWv = 5,2 * (2,4 * 2, 2) = 5, 2 * 4,2 = 5, 4
